{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "RND_PPO_cont_ftr_nsn_mtCar_php.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "TPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "Barcva7d_jwo",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "\"\"\"\n",
    "Random Network Distillation (RND) with Proximal Policy Optimization (PPO) implentation in Tensorflow.\n",
    "This is a continuous action version which solves the mountain car continuous problem (MountainCarContinuous-v0).\n",
    "The RND helps learning with curiosity driven exploration.\n",
    "\"\"\"\n",
    "\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.preprocessing\n",
    "import tensorflow as tf\n",
    "# Approximates feature map of an RBF kernel by Monte Carlo approximation of its Fourier transform.\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html\n",
    "from sklearn.kernel_approximation import RBFSampler \n",
    "tf.compat.v1.disable_eager_execution()\n"
   ],
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RiD9RHbWp_KX",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "class RunningStats(object):\n",
    "    # This class which computes global stats is adapted & modified from:\n",
    "    # https://github.com/openai/baselines/blob/master/baselines/common/running_mean_std.py\n",
    "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
    "    def __init__(self, epsilon=1e-4, shape=()):\n",
    "        self.mean = np.zeros(shape, 'float64')\n",
    "        self.var = np.ones(shape, 'float64')\n",
    "        self.std = np.ones(shape, 'float64')\n",
    "        self.count = epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_var = np.var(x, axis=0)\n",
    "        batch_count = x.shape[0]\n",
    "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
    "\n",
    "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
    "        delta = batch_mean - self.mean\n",
    "        \n",
    "        new_mean = self.mean + delta * batch_count / (self.count + batch_count)\n",
    "        m_a = self.var * self.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / (self.count + batch_count)\n",
    "        new_var = M2 / (self.count + batch_count)\n",
    "\n",
    "        self.mean = new_mean\n",
    "        self.var = new_var\n",
    "        self.std = np.maximum(np.sqrt(self.var), 1e-6)\n",
    "        #self.std = np.sqrt(np.maximum(self.var, 1e-2))                            \n",
    "        self.count = batch_count + self.count"
   ],
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mmBujl2H_urf",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "class RND_PPO(object):            \n",
    "    def __init__(self, tf_operation_level_seed):\n",
    "        \n",
    "        self.s = tf.compat.v1.placeholder(tf.float32, [None, S_DIM], 'state')\n",
    "        self.s_ = tf.compat.v1.placeholder(tf.float32, [None, next_S_DIM], 'state_')\n",
    "        \n",
    "        # RND        \n",
    "        with tf.compat.v1.variable_scope('RND'):\n",
    "          with tf.compat.v1.variable_scope('target'):\n",
    "            r_w = tf.random_normal_initializer(seed=tf_operation_level_seed)       \n",
    "            # Fixed target network encodes state to features        \n",
    "            # Network randomly initialized once but never trained, params remain fixed, trainable=False  \n",
    "            self.target_out = tf.compat.v1.layers.dense(self.s_, encode_features, kernel_initializer = r_w, name='target_out', trainable=False)            \n",
    "            #hidden_layer = tf.keras.layers.Dense(self.s_, num_hidden, tf.nn.relu, kernel_initializer = r_w, name='t_hidden', trainable=False)             \n",
    "            #self.target_out = tf.keras.layers.Dense(hidden_layer, encode_features, kernel_initializer = r_w, name='target_out', trainable=False)\n",
    "            \n",
    "        # predictor network\n",
    "          with tf.compat.v1.variable_scope('predictor'):\n",
    "            #p_w = tf.zeros_initializer()\n",
    "            p_w = tf.random_normal_initializer(seed=tf_operation_level_seed+1)\n",
    "            #p_w = tf.glorot_uniform_initializer(seed=tf_operation_level_seed+1)   \n",
    "            self.predictor_out = tf.compat.v1.layers.dense(self.s_, encode_features, kernel_initializer = p_w, name='predictor_out', trainable=True)            \n",
    "            #hidden_layer = tf.keras.layers.Dense(self.s_, num_hidden, tf.nn.relu, kernel_initializer = p_w, name='p_hidden', trainable=True) \n",
    "            #self.predictor_out = tf.keras.layers.Dense(hidden_layer, encode_features, kernel_initializer = p_w, name='predictor_out', trainable=True)\n",
    "            # self.predictor_loss is also the intrinsic reward\n",
    "            self.predictor_loss = tf.reduce_sum(tf.square(self.target_out - self.predictor_out), axis=1)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('PPO'):        \n",
    "          # critic\n",
    "          with tf.compat.v1.variable_scope('critic'):\n",
    "            #c_w = tf.zeros_initializer()\n",
    "            c_w = tf.random_normal_initializer(seed=tf_operation_level_seed+2)\n",
    "            #c_w = tf.glorot_uniform_initializer(seed=tf_operation_level_seed+2)  \n",
    "            with tf.compat.v1.variable_scope('critic_extrinsic'):\n",
    "                # critic network for extrinsic reward\n",
    "                self.v = tf.compat.v1.layers.dense(self.s, 1, kernel_initializer = c_w, name='val', trainable=True)              \n",
    "                #hidden_layer = tf.keras.layers.Dense(self.s, num_hidden, tf.nn.relu, kernel_initializer = c_w, name='c_e_hidden', trainable=True) \n",
    "                #self.v = tf.keras.layers.Dense(hidden_layer, 1, kernel_initializer = c_w, name='val', trainable=True)            \n",
    "                self.tfdc_r = tf.compat.v1.placeholder(tf.float32, [None, 1], 'discounted_r')\n",
    "                self.advantage = self.tfdc_r - self.v\n",
    "                self.closs = tf.reduce_mean(tf.square(self.advantage))            \n",
    "            with tf.compat.v1.variable_scope('critic_intrinsic'):\n",
    "                # critic network for intrinsic reward           \n",
    "                self.v_i = tf.compat.v1.layers.dense(self.s, 1, kernel_initializer = c_w, name='val_i', trainable=True)                      \n",
    "                #hidden_layer = tf.keras.layers.Dense(self.s, num_hidden, tf.nn.relu, kernel_initializer = c_w, name='c_i_hidden', trainable=True) \n",
    "                #self.v_i = tf.keras.layers.Dense(hidden_layer, 1, kernel_initializer = c_w, name='val_i', trainable=True)                                  \n",
    "                self.tfdc_r_i = tf.compat.v1.placeholder(tf.float32, [None, 1], 'discounted_r_i')\n",
    "                self.advantage_i = self.tfdc_r_i - self.v_i\n",
    "                self.closs_i = tf.reduce_mean(tf.square(self.advantage_i))\n",
    "            with tf.compat.v1.variable_scope('total_critic_loss'):\n",
    "                self.total_closs = self.closs + self.closs_i\n",
    "            \n",
    "          # actor                     \n",
    "          with tf.compat.v1.variable_scope('actor'):\n",
    "            pi, pi_params = self._build_anet('pi', trainable=True)\n",
    "            oldpi, oldpi_params = self._build_anet('oldpi', trainable=False) # trainable=False\n",
    "            with tf.compat.v1.variable_scope('sample_action'):\n",
    "              self.sample_op = tf.squeeze(pi.sample(1), axis=0) # choosing action\n",
    "            with tf.compat.v1.variable_scope('update_oldpi'):\n",
    "              self.update_oldpi_op = [oldp.assign(p) for p, oldp in zip(pi_params, oldpi_params)]\n",
    "            with tf.compat.v1.variable_scope('surrogate_actor_loss'):\n",
    "              self.tfa = tf.compat.v1.placeholder(tf.float32, [None, A_DIM], 'action')\n",
    "              self.tfadv = tf.compat.v1.placeholder(tf.float32, [None, 1], 'advantage')                 \n",
    "              ratio = pi.prob(self.tfa) / oldpi.prob(self.tfa)\n",
    "              surr = ratio * self.tfadv\n",
    "              self.aloss = -tf.reduce_mean(tf.minimum(surr,\n",
    "                                                      tf.clip_by_value(ratio, 1.-epsilon, 1.+epsilon)*self.tfadv))\n",
    "\n",
    "            with tf.compat.v1.variable_scope('entropy'):                \n",
    "              entropy = -tf.reduce_mean(pi.entropy()) # Compute the differential entropy of the multivariate normal.                   \n",
    "                            \n",
    "        with tf.compat.v1.variable_scope('total_loss'):\n",
    "            self.total_loss = tf.reduce_mean(self.predictor_loss) + self.total_closs*c_loss_coeff + self.aloss\n",
    "        with tf.compat.v1.variable_scope('train'):\n",
    "            self.train_op = tf.compat.v1.train.AdamOptimizer(TL_LR).minimize(self.total_loss + entropy * ENTROPY_BETA, \n",
    "                                                                   global_step=tf.compat.v1.train.get_global_step())\n",
    "        with tf.compat.v1.variable_scope('sess'):\n",
    "            self.sess = tf.compat.v1.Session()              \n",
    "            self.sess.run(tf.compat.v1.global_variables_initializer())\n",
    "            \n",
    "    def update(self, s, s_, a, r, r_i, adv):\n",
    "        if state_ftr == True:\n",
    "            s = featurize_batch_state(s)        \n",
    "        self.sess.run(self.update_oldpi_op)        \n",
    "        [self.sess.run(self.train_op, {self.s: s, self.tfa: a, self.tfadv: adv, self.tfdc_r: r, self.tfdc_r_i: r_i, self.s_: s_}) for _ in range(EPOCH)]\n",
    "            \n",
    "    def _build_anet(self, name, trainable):\n",
    "        # tanh range = [-1,1]\n",
    "        # softplus range = {0,inf}\n",
    "        with tf.compat.v1.variable_scope(name):          \n",
    "            #a_w = tf.zeros_initializer()\n",
    "            #a_w = tf.random_normal_initializer(seed=tf_operation_level_seed+3) # can't use random for actor, produces nan action\n",
    "            a_w = tf.compat.v1.glorot_uniform_initializer(seed=tf_operation_level_seed+3)\n",
    "            mu = tf.compat.v1.layers.dense(self.s, A_DIM, tf.nn.tanh, kernel_initializer = a_w, name='mu', trainable=trainable) \n",
    "            sigma = tf.compat.v1.layers.dense(self.s, A_DIM, tf.nn.softplus, kernel_initializer = a_w, name='sigma', trainable=trainable) + 1e-4            \n",
    "            #hidden_layer = tf.keras.layers.Dense(self.s, num_hidden, tf.nn.relu, kernel_initializer = a_w, name='a_hidden', trainable=trainable) \n",
    "            #mu = tf.keras.layers.Dense(hidden_layer, A_DIM, tf.nn.tanh, kernel_initializer = a_w, name='mu', trainable=trainable) \n",
    "            #sigma = tf.keras.layers.Dense(hidden_layer, A_DIM, tf.nn.softplus, kernel_initializer = a_w, name='sigma', trainable=trainable) + 1e-4\n",
    "            norm_dist = tf.compat.v1.distributions.Normal(loc=mu, scale=sigma)\n",
    "        params = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope=name)\n",
    "        return norm_dist, params\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        if state_ftr == True:\n",
    "          s = featurize_state(s)\n",
    "        s = s[np.newaxis, :]\n",
    "        a = self.sess.run(self.sample_op, {self.s: s})[0]            \n",
    "        return np.clip(a, env.action_space.low[0], env.action_space.high[0])\n",
    "\n",
    "    def get_v(self, s):\n",
    "        if state_ftr == True:\n",
    "          s = featurize_state(s)\n",
    "        if s.ndim < 2: s = s[np.newaxis, :]\n",
    "        return self.sess.run(self.v, {self.s: s})[0, 0]\n",
    "      \n",
    "    def get_v_i(self, s):\n",
    "        if state_ftr == True:\n",
    "          s = featurize_state(s)\n",
    "        if s.ndim < 2: s = s[np.newaxis, :]\n",
    "        return self.sess.run(self.v_i, {self.s: s})[0, 0]\n",
    "      \n",
    "    def intrinsic_r(self, s_):\n",
    "        return self.sess.run(self.predictor_loss, {self.s_: s_})      \n",
    "      \n",
    "    def add_vtarg_and_adv(self, R, done, V, v_s_, gamma, lam):\n",
    "        # This function is adapted & modified from:\n",
    "        # https://github.com/openai/baselines/blob/master/baselines/ppo1/pposgd_simple.py\n",
    "        # Compute target value using TD(lambda) estimator, and advantage with GAE(lambda)\n",
    "        # last element is only used for last vtarg, but we already zeroed it if last new = 1\n",
    "        done = np.append(done, 0)\n",
    "        V_plus = np.append(V, v_s_)\n",
    "        T = len(R)\n",
    "        adv = gaelam = np.empty(T, 'float32')\n",
    "        lastgaelam = 0\n",
    "        for t in reversed(range(T)):\n",
    "            nonterminal = 1-done[t+1]        \n",
    "            delta = R[t] + gamma * V_plus[t+1] * nonterminal - V_plus[t]\n",
    "            gaelam[t] = lastgaelam = delta + gamma * lam * nonterminal * lastgaelam   \n",
    "        tdlamret = np.vstack(adv) + V\n",
    "        return tdlamret, adv # tdlamret is critic_target or Qs         "
   ],
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Md2IB4tgKNBw",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# pre-train updating of s_ to running_stats_s_ object to prepare for normalization\n",
    "def state_next_normalize(sample_size, running_stats_s_):\n",
    "  buffer_s_ = []\n",
    "  s = env.reset()  \n",
    "  for i in range(sample_size):\n",
    "    a = env.action_space.sample()\n",
    "    s_, r, done, _ = env.step(a)\n",
    "    buffer_s_.append(s_)    \n",
    "  running_stats_s_.update(np.array(buffer_s_))"
   ],
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kK3hfBq0vkST",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# normalize & clip a running buffer\n",
    "# used on extrinsic reward (buffer_r), intrinsic reward (buffer_r_i) & next state(s_)\n",
    "def running_stats_fun(run_stats, buf, clip, clip_state):\n",
    "    run_stats.update(np.array(buf))\n",
    "    buf = (np.array(buf) - run_stats.mean) / run_stats.std   \n",
    "    if clip_state == True:\n",
    "      buf = np.clip(buf, -clip, clip)\n",
    "    return buf"
   ],
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9z2_28nlCqq7",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "start_time = time.time()\n",
    "\n",
    "EP_MAX = 200 \n",
    "EP_LEN = 128\n",
    "\n",
    "sample_size = 10000\n",
    "n_comp = 100 \n",
    "\n",
    "ext_r_coeff = 2 # extrinsic reward coefficient\n",
    "c_loss_coeff = 0.5 # coefficient for total critic loss\n",
    "\n",
    "s_CLIP = 10 # clip for state buffer\n",
    "next_s_CLIP = 5 # clip for next state's buffer\n",
    "r_CLIP = 1 # clip for extrinsic reward, note that intrinsic rewards are not clip.\n",
    "\n",
    "encode_features = 10000 # number of output for RND predictor's network\n",
    "\n",
    "# Hyper parameters for computation of TD lambda return and policy advantage using GAE\n",
    "GAMMA = 0.999 #0.95 #0.95 # discount factor for extrinsic reward\n",
    "GAMMA_i = 0.99 #0.95 # discount factor for intrinsic reward\n",
    "lamda = 0.95\n",
    "\n",
    "# PPO Hyper parameters\n",
    "TL_LR = 0.0001 #0.001\n",
    "BATCH = 4 #4 10\n",
    "EPOCH = 4 #4 10\n",
    "epsilon = 0.1\n",
    "ENTROPY_BETA = 0.001#0.001 \n",
    "\n",
    "num_hidden = 64\n",
    "\n",
    "np_random_seed = 1\n",
    "tf_graph_level_seed = np_random_seed + 1\n",
    "tf_operation_level_seed = tf_graph_level_seed + 1\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "state_ftr = True # True False\n",
    "if state_ftr == True:\n",
    "    S_DIM, next_S_DIM, A_DIM = n_comp*4, 2, 1 # MountainCarContinuous-v0, featurize s only   \n",
    "else:    \n",
    "    S_DIM, next_S_DIM, A_DIM = 2, 2, 1 # MountainCarContinuous-v0, not featurize\n",
    "    \n",
    "state_next_normal = True # normalize s_ for RND's target & predictor networks"
   ],
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9zjCoG5YDAaV",
    "colab_type": "code",
    "outputId": "736fa235-52eb-4522-b28c-bb6c0d9df03f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "np.random.seed(np_random_seed)\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "tf.compat.v1.set_random_seed(tf_graph_level_seed) # called before session creation\n",
    "global_step = tf.compat.v1.Variable(0, name=\"global_step\", trainable=False)\n",
    "rnd_ppo = RND_PPO(tf_operation_level_seed)\n",
    "\n",
    "all_steps = [] # stores number of episodic steps for display\n",
    "all_ep_r = [] # stores raw episodic rewards for display\n",
    "mv_all_ep_r = [] # store moving average episodic rewards for display\n",
    "\n",
    "hit_counter = 0 # number of times flag reached by agent\n",
    "\n",
    "# normalization objects\n",
    "running_stats_s = RunningStats()\n",
    "running_stats_s_ = RunningStats()\n",
    "running_stats_r = RunningStats()\n",
    "running_stats_r_i = RunningStats()"
   ],
   "execution_count": 36,
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-cf8a87ec0ede>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_graph_level_seed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# called before session creation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"global_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrnd_ppo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRND_PPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_operation_level_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mall_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# stores number of episodic steps for display\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-69c93cb6798e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tf_operation_level_seed)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             self.train_op = tf.compat.v1.train.AdamOptimizer(TL_LR).minimize(self.total_loss + entropy * ENTROPY_BETA, \n\u001b[0;32m---> 76\u001b[0;31m                                                                    global_step=tf.compat.v1.train.get_global_step())\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sess'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pommerman/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py\u001b[0m in \u001b[0;36mget_global_step\u001b[0;34m(graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0massert_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pommerman/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py\u001b[0m in \u001b[0;36massert_global_step\u001b[0;34m(global_step_tensor)\u001b[0m\n\u001b[1;32m    178\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_integer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     raise TypeError('Existing \"global_step\" does not have integer type: %s' %\n\u001b[0;32m--> 180\u001b[0;31m                     global_step_tensor.dtype)\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m   if (global_step_tensor.get_shape().ndims != 0 and\n",
      "\u001b[0;31mTypeError\u001b[0m: Existing \"global_step\" does not have integer type: <dtype: 'resource'>"
     ],
     "ename": "TypeError",
     "evalue": "Existing \"global_step\" does not have integer type: <dtype: 'resource'>",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Yhkk54T4C2tm",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "if state_ftr == True:\n",
    "# ----------\n",
    "    # The following code for state featurization is adapted & modified from dennybritz's repository located at:\n",
    "    # https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb\n",
    "    # Feature Preprocessing: Normalize to zero mean and unit variance\n",
    "    # We use a few samples from the observation space to do this\n",
    "    states = np.array([env.observation_space.sample() for x in range(sample_size)]) # pre-trained, states preprocessing\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    scaler.fit(states) # Compute the mean and std to be used for later scaling.\n",
    "\n",
    "    # convert states to a featurizes representation.\n",
    "    # We use RBF kernels with different variances to cover different parts of the space\n",
    "    featurizer = sklearn.pipeline.FeatureUnion([ # Concatenates results of multiple transformer objects.       \n",
    "            (\"rbf1\", RBFSampler(gamma=5.0, n_components=n_comp)),\n",
    "            (\"rbf2\", RBFSampler(gamma=2.0, n_components=n_comp)),\n",
    "            (\"rbf3\", RBFSampler(gamma=1.0, n_components=n_comp)),\n",
    "            (\"rbf4\", RBFSampler(gamma=0.5, n_components=n_comp))      \n",
    "            ])\n",
    "    featurizer.fit(\n",
    "        scaler.transform(states)) # Perform standardization by centering and scaling\n",
    "\n",
    "    # state featurization of state(s) only, \n",
    "    # not used on s_ for RND's target & predictor networks\n",
    "    def featurize_state(state):     \n",
    "        scaled = scaler.transform([state]) # Perform standardization by centering and scaling                    \n",
    "        featurized = featurizer.transform(scaled) # Transform X separately by each transformer, concatenate results.\n",
    "        return featurized[0]\n",
    "# ----------\n",
    "    def featurize_batch_state(batch_states):\n",
    "        fs_list = []\n",
    "        for s in batch_states:\n",
    "            fs = featurize_state(s)\n",
    "            fs_list.append(fs)    \n",
    "        return fs_list   "
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "w3T7ZQIbR7hE",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "if state_next_normal == True:\n",
    "  state_next_normalize(sample_size, running_stats_s_)"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-46db95d30801>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstate_next_normal\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mstate_next_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_stats_s_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'running_stats_s_' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'running_stats_s_' is not defined",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "86maMUFDDIs_",
    "colab_type": "code",
    "outputId": "6ac45342-7d3a-4eec-dccc-295cc44abf4a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "for ep in range(EP_MAX):\n",
    "    s = env.reset()\n",
    "    buffer_s, buffer_a, buffer_r, buffer_s_, buffer_done, buffer_V, buffer_V_i = [], [], [], [], [], [], []\n",
    "    ep_r = 0\n",
    "    steps = 0\n",
    "    for t in itertools.count(): # each rollout\n",
    "        #env.render()               \n",
    "        a = rnd_ppo.choose_action(s)\n",
    "        s_, r, done, _ = env.step(a)\n",
    "                \n",
    "        buffer_s.append(s)\n",
    "        buffer_a.append(a)\n",
    "        buffer_r.append(ext_r_coeff * r)    \n",
    "        buffer_s_.append(s_)\n",
    "        buffer_done.append(done)                  \n",
    "        \n",
    "        v = rnd_ppo.get_v(s)\n",
    "        buffer_V.append(v)  \n",
    "        v_i = rnd_ppo.get_v_i(s)\n",
    "        buffer_V_i.append(v_i)  \n",
    "        \n",
    "        s = s_\n",
    "        ep_r += r\n",
    "        steps += 1                            \n",
    "        \n",
    "        # update rnd_ppo\n",
    "        if (t+1) % BATCH == 0 or t == EP_LEN-1:\n",
    "            buffer_s_ = running_stats_fun(running_stats_s_, buffer_s_, s_CLIP, True)          \n",
    "            if state_next_normal == True:\n",
    "                buffer_s_ = running_stats_fun(running_stats_s_, buffer_s_, next_s_CLIP, True)                \n",
    "            buffer_r_i = rnd_ppo.intrinsic_r(buffer_s_)            \n",
    "            buffer_r = running_stats_fun(running_stats_r, buffer_r, r_CLIP, True)            \n",
    "            buffer_r_i = running_stats_fun(running_stats_r_i, buffer_r_i, r_CLIP, False)\n",
    "            \n",
    "            v_s_ = rnd_ppo.get_v(s_)                                 \n",
    "            tdlamret, adv = rnd_ppo.add_vtarg_and_adv(np.vstack(buffer_r), \n",
    "                                                  np.vstack(buffer_done), \n",
    "                                                  np.vstack(buffer_V), \n",
    "                                                  v_s_, \n",
    "                                                  GAMMA, \n",
    "                                                  lamda)\n",
    "            v_s_i = rnd_ppo.get_v_i(s_)                     \n",
    "            tdlamret_i, adv_i = rnd_ppo.add_vtarg_and_adv(np.vstack(buffer_r_i),\n",
    "                                                      np.vstack(buffer_done), \n",
    "                                                      np.vstack(buffer_V_i), \n",
    "                                                      v_s_i, \n",
    "                                                      GAMMA_i, \n",
    "                                                      lamda)\n",
    "\n",
    "            bs, bs_, ba, br, br_i, b_adv = np.vstack(buffer_s), np.vstack(buffer_s_), np.vstack(buffer_a), tdlamret, tdlamret_i, np.vstack(adv + adv_i) # sum advantages\n",
    "            buffer_s, buffer_a, buffer_r, buffer_s_, buffer_done, buffer_V, buffer_V_i = [], [], [], [], [], [], []\n",
    "            rnd_ppo.update(bs, bs_, ba, br, br_i, b_adv)\n",
    "            \n",
    "        if done:            \n",
    "            print('done', ep, steps, ep_r) \n",
    "            if r > 0:\n",
    "                hit_counter += 1\n",
    "                print('********** r > 0 **********', hit_counter, hit_counter / (ep+1))#, ENTROPY_BETA)\n",
    "                #if ENTROPY_BETA > 0.01:\n",
    "                #    ENTROPY_BETA = ENTROPY_BETA * 0.9\n",
    "            break    \n",
    "\n",
    "    if ep == 0: \n",
    "        mv_all_ep_r.append(ep_r)\n",
    "    else: \n",
    "        mv_ep_r = mv_all_ep_r[-1]*0.9 + ep_r*0.1\n",
    "        print('          mv_ep_r', mv_ep_r)        \n",
    "        mv_all_ep_r.append(mv_ep_r)       \n",
    "      \n",
    "    all_ep_r.append(ep_r)      \n",
    "    all_steps.append(steps)            \n",
    "    \n",
    "print('hit_counter', hit_counter, hit_counter/EP_MAX)    \n",
    "\n",
    "plt.plot(all_steps)\n",
    "plt.ylabel(\"steps\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(all_ep_r)\n",
    "plt.ylabel(\"reward\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(mv_all_ep_r)\n",
    "plt.ylabel(\"mv_reward\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.show()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))        "
   ],
   "execution_count": 11,
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8ff2f23835b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# each rollout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m#env.render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd_ppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0ms_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rnd_ppo' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'rnd_ppo' is not defined",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}